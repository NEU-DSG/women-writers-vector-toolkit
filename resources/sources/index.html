<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/js/bootstrap.min.js" integrity="sha384-o+RDsa0aLu++PJvFqy8fFScvbHFLtbvScb8AjopnFD+iEQ7wo/CG0xlczd+2O/em" crossorigin="anonymous"></script>
  <link rel="stylesheet"
    href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.2/css/bootstrap.min.css"
    integrity="sha384-Smlep5jCw/wG7hdkwQ/Z5nLIefveQRIY9nfy6xoR1uRYBtpZgI6339F5dgvm/e9B" crossorigin="anonymous"></link>
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet"></link>
  <link rel="stylesheet" href="../../styles/main.css"></link>
  <link rel="stylesheet" href="../../styles/sources.css"></link>
</head>

<body>
  <header>
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
      <div class="container">
        <div class="d-inline-flex">
          <img src="../../assets/logo.png" width="61" height="52" class="d-inline-block align-top" alt="">
          <a href="../../index.html" id="wwvt-home">Women Writers Vector Toolkit</a>
        </div>
        <div>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
              <li class="nav-item">
                <div class="btn-group">
                  <div class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    About
                  </div>
                  <div class="dropdown-menu">
                    <a class="dropdown-item" href="../../about/background.html">Background</a>
                    <a class="dropdown-item" href="../../about/how-to-navigate.html">How to Navigate</a>
                    <a class="dropdown-item" href="../../about/team-members.html">Team Members</a>
                  </div>
                </div>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="#">Tool</a>
              </li>
              <li class="nav-item">
                <div class="btn-group">
                  <div class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    Resources
                  </div>
                  <div class="dropdown-menu">
                    <a class="dropdown-item" href="../glossary/index.html">Glossary</a>
                    <a class="dropdown-item" href="#">Case Studies</a>
                    <a class="dropdown-item" href="../sources/index.html">Helpful Sources</a>
                  </div>
                  <div class="dropdown-menu">
                    ...
                  </div>
                </div>
              </li>
              <li class="nav-item">
                <div class="btn-group">
                  <div class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    Teaching &amp; Exploration
                  </div>
                  <div class="dropdown-menu">
                    <a class="dropdown-item" href="../../teaching-exploration/index.html">Teaching with this Tool</a>
                    <a class="dropdown-item" href="#">Suggested Searches</a>
                    <a class="dropdown-item" href="../../teaching-exploration/assignments/index.html">Assignments</a>
                  </div>
                  <div class="dropdown-menu">
                    ...
                  </div>
                </div>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="../../methodology/index.html">Methodology</a>
              </li>
              
            </ul>
          </div>
          
          
        </div>
      </div>
    </nav>
  </header>
  <div class="main-body">
  <div class="wwp-text">
  <p>This annotated bibliography is a list of helpful sources that have been selected, annotated, and organized by the Pedagogy Team. They are broken into four main categories to make for easier navigation: introductory readings, advanced readings, background readings, and digital pedagogy readings.</p><p>The introductory readings center are more approachable readings either explicitly about word embeddings or similar corpus-based methodologies. The advanced readings go into further detail about the actual methods used, looking more specifically at the code and mathematical background. The background readings provide a theoretical foundation for our own project’s methodology. Finally, the digital pedagogy readings are helpful readings from practitioners who are either providing a case study about or a theoretical model for incorporating digital technologies in their classroom.</p>
  </div>
  <button class="sources-collapsible">Introductory Readings</button>
  <div class="sources-content">
  <br>
  <p class="sources-hangindent"><b>Cherny, Lynn. "<a href="http://blogger.ghostweather.com/2014/11/visualizing-word-embeddings-in-pride.html">Visualizing Word Embeddings in Pride and Prejudice.</a>” <i>Ghostweather Research and Design Blog</i>, 22 Nov. 2014.</b></p>
  <p class="sources-textindent">This reading is an accessible process-oriented reflection on training a word vector model and presenting that model using Javascript; Cherny demonstrates the values of demonstration and play in her reflection <a href="http://www.ghostweather.com/files/word2vecpride/">creating this model</a>. Cherny trained a model on the full corpus of Austen novels and then replaced all of the nouns in Pride and Prejudice with the most similar word for each; an associated visualization fills in the noun pairs and the path between them in two-dimensional space when they are moused over, resulting in a cumulative word-cloud developed by the viewer's exploration of terms. Cherny also discusses her results for gendered words in the Austen novel (the word most closely related to "husband" is "nerves"). Cherny’s final set of bullet points demonstrates the journey of her choices and discoveries by playing with her model and the texts. </p>
    <br>
      <p class="sources-hangindent"><b>Gavin, Michael. “<a href="http://modelingliteraryhistory.org/2015/09/18/the-arithmetic-of-concepts-a-response-to-peter-de-bolla/">The Arithmetic of Concepts: A Response to Peter de Bolla</a>.” <i>Modeling Literary History</i>, 18 Sept. 2015.</b></p>
      <p class="sources-textindent">Gavin was inspired by Peter de Bolla's Architecture of Concepts (Fordham University Press, 2013), which showed how the conceptual meaning of the term ‘rights’ shifted over the eighteenth century.” De Bolla used manual keyword searches in his research, so Gavin created this response to outline computational approaches to the same research question. He uses the EEBO-TCP corpus as a point of comparison for the 18 uses of "rights" in the work of John Locke's Two treatises of government (1690). The first few sections provide a useful overview of the basics of semantic analysis using word vectors, but Gavin admits a lack of technical understanding between “syntagmatic” and “paradigmatic” similarity in semantic models. From this overview, the essay moves into the actual analysis using Gavin's own R package, “<a href="https://github.com/michaelgavin/empson">Empson</a>.” The result of his quantitative semantic analysis ultimately confirm the ambiguity of the term “rights” (a claim shared also by de Bolla and earlier traditional humanists). The excitement over these computational methods is then not about new discoveries, but rather about the parallels between “concepts” that appear through mathematical manipulation of the vector space and the traditional linguistic theory of concepts as patterns of word usage across human networks.” Ultimately, it seems like Gavin sees vector space analysis as a way to reveal this pattern of word use.</p>
      <br/>
      <p class="sources-textindent"><b>Heuser, Ryan. “<a href="http://ryanheuser.org/word-vectors-1/.">Word Vectors in the Eighteenth Century, Episode 1: Concepts</a>.” <i>Virtue and the Virtual</i>, 14 Apr. 2016.</b></p>
      <p class="sources-textindent">	Heuser introduces the subject of word vectors to the reader in his first blog and includes a handful of detailed analyses of 18th century prose to exemplify how word vector analysis can be useful for early modern texts. Structured more like a conversation over coffee than a review of or research article on word2vec, Heuser solidifies both the practice and the idea of using word vectors through his own amateur lens of early understanding of the tool.</p>
      <br/>
      <p class="sources-hangindent"><b>-----. “<a href="http://ryanheuser.org/word-vectors-2/">Word Vectors in the Eighteenth Century, Episode 2: Methods</a>.” <i>Virtue and the Virtual</i>, 1 June 2016.</b></p>
      <p class="sources-textindent">Heuser’s post explains the conceptual logic behind word vector models. Within a model, the similarity of two words is a result of how and where they occur together across the input data, and thus word embedding models take into account contextual information and semantic similarities or differences. The process by which the words become represented as vectors and then embedded into the model is different across different algorithms and software, and Heuser admits that the intelligibility of the underlying math is one of the downfalls of word embedding models. To illustrate the function of word vector models, he outlines the semantic relationships between Queen/Woman and King/Man, then their vector relationship, to reveal how and why the mathematical process performed by the vector algorithms produces the same result as human logic. Lastly, Heuser includes a discussion of why he thinks a “skipgram” size of 10 might be optimal for his ECCO-TCP corpus.</p>
      <br/>
      <p class="sources-hangindent"><b>Klein, Lauren. F. “<a href="https://doi.org/10.1215/00029831-2367310">The Image of Absence: Archival Silence, Data Visualization, and James Hemings</a>.” <i>American Literature</i>, vol. 85, no. 4, Jan. 2013, pp. 661–88.</b></p>
      <p class="sources-textindent">Although Klein does not use word vector models specifically, this piece provides a valuable framework for making explicit discursive reading practices in creating data models and visualizations . Klein addresses archival silence, or the erasure of particular narratives and people in recording and archival work, to make visible the presence of James Hemings, Sally Heming’s brother and a slave owned by Thomas Jefferson, in Jefferson’s digitized letter correspondences. Klein presents the steps she took to create a social network visualization to capture a more nuanced visualization of the relationships between the people mentioned in these correspondents.</p>
      <br/>
      <p class="sources-hangindent"><b>Recchia, Gabriel. “‘<a href="http://www.twonewthings.com/gabrielrecchia/2016/06/11/numberless-degrees-of-similitude-word-vectors/">Numberless Degrees of Similitude’: A Response to Ryan Heuser’s ‘Word Vectors in the Eighteenth Century, Part 1.’</a>” <i>Gabriel Recchia</i>, 11 June 2016.</b></p>
      <p class="sources-textindent">Recchia begins with an explanation of the predecessors for algorithmic vector space models and an articulation of the caution that digital humanists should know how to interpret findings from words vector models, what information is conveyed by transforming language into statistics, and when/where that statistical representation would not be valid. Recchia then defines four different types of models and training methods: count-based models, random vector models, the “continuous bag of words” method, and the “skip-gram” algorithm. The bulk of Recchia’s post examines the same relationship between “genius,” “learning,” “virtue,” and “riches” that Ryan Heuser explores in “Word Vectors in the Eighteenth Century” (see above) in order to elucidate the complexities and limits of word vector analysis. Ultimately, Recchia concludes with a claim that computational methods can help focus research, but that findings should be confirmed by close-reading methods.</p>
      <br/>
      <p class="sources-hangindent"><b>Schmidt, Ben. “<a href="http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html">Vector Space Models for the Digital Humanities</a>.” <i>Ben Schmidt</i>, 25 Oct. 2015.</b></p>
      <p class="sources-textindent">In his post, Schmidt provides a few major reasons why WEMs, or word embedding models, are so useful for a deeper analysis of word relationships. He clearly defines what WEMs are, and includes some examples from his own work to clearly delineate word2vec analysis from scatterplots. However, with the formality and DH jargon that are used in the post, this is more for a reader who is familiar with the DH world, but wants to know more about WEMs, than someone first stepping into the world of digital humanities.</p>
      <br/>
      <p class="sources-hangindent"><b>-----. “<a href="http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html">Rejecting the Gender Binary: A Vector-Space Operation</a>.” <i>Ben Schmidt</i>, 30 Oct. 2015.</b></p>
      <p class="sources-textindent">In this essay, Schmidt uses word embedding models to isolate the vectors associated with gendered words in teaching reviews from Rate my Professor. He also uses a process called “vector subtraction” or “vector rejection” in which he removes the vectors of some words from another word’s vector space to better focus on the relationship between the gender binary and particular words. The results demonstrates the existence of gendered language and how this language can reinscribe harmful gender binaries in academia. This reading is useful because it demonstrates how textual analysis can make explicit social biases in data, provides a step-by-step walkthrough for how Schmidt created this model along with the actual scripts, and explains the results.</p>
      <br/>
      <p class="sources-hangindent"><b>“<a href="https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example">How Does Word2vec Work? Can Someone Walk through a Specific Example?</a>” <i>Quora</i>, 20 Oct. 2014.</b></p>
      <p class="sources-textindent">This Quora thread provides a multiplicity of answers and a well-rounded explanation of the word2vec algorithm. Omer Levy gives a brief overview, Ajit Rajaskharan provides detailed commentary on the connection between the source code, Borislav Agapiev elaborates on the probability statistics behind the code, Stephan Gouws explains the geometrical theories of the vector space, and Abhishek Patnia clarifies the underlying assumptions from a Natural Language Processing (NLP) perspective. Overall, this is likely the most useful, publically-available resource for non-computer scientists to grasp an understanding of the algorithm itself. </p>
</div>
    &nbsp;
    <button class="sources-collapsible">Advanced Readings</button>
    <div class="sources-content">
      <br/>
      <p class="sources-hangindent"><b>Allison, Sarah, et al. “<a href="https://litlab.stanford.edu/LiteraryLabPamphlet1.pdf">Quantitative Formalism: An Experiment</a>.” <i>Stanford Literary Lab Pamphlet</i>, vol. 1, Jan. 2011.</b></p>
      <p class="sources-textindent">This study provides a brief explanation of how algorithms understand language, the limits to that computational understanding, and the possibilities for digital humanists to put it to work. The authors are interested in whether formalistic features of literary works (like genre) can be determined via quantitative methods. The two tools used here, Docuscope and Most Frequent Words, both rely on an unsupervised factor analysis using Language Action Types (LATs)--which is a smart dictionary that can determine a word’s function. The researchers concluded that language and style were not enough to delimit one genre from another, and, though the systems tracked features that makes one genre different from another, this tells us little about a form’s inner structure.</p>
      <br/>
      <p class="sources-hangindent"><b>Gagliano, Andrea, et al. “<a href="https://pdfs.semanticscholar.org/0dd6/e7ff83249edc951b22806779fc4946432f02.pdf?_ga=2.78675284.1735952909.1526869615-1307720411.1526869615">Intersecting Word Vectors to Take Figurative Language to New Heights</a>.” Presented at <i>Fifth Workshop on Computational Linguistics for Literature</i>. 2016.</b></p>
      <p class="sources-textindent">Gagilano et al use word embedding to create a model of metaphors to better develop systems to understand how figurative language is used in poetry. Because metaphors are created by connecting two words or concepts to represent another idea, a statistical model of metaphors should create a set of connector words that articulates the “figurative relationship” between two word pairs. They explain how they were able to model connector words using word pairs and the basic word vector relationships like addition, intersection, and subtraction. They also use a case study to show how they might qualitatively analyze the results of this method. This article demonstrates how an intimate understanding of the word embeddings process can lead to different methods and approaches to exploring relationships among words.</p>
      <br/>
    </div>
    &nbsp;
    <button class="sources-collapsible">Background Readings</button>
    <div class="sources-content">
      <br/>
      <p class="sources-hangindent"><b>Jockers, Matthew, and Julia Flanders. “<a href="https://digitalcommons.unl.edu/englishfacpubs/106/">A Matter of Scale</a>.” Keynote for <i>Boston Area Days of Digital Humanities Conference</i>, 2013.</b></p>
      <p class="sources-textindent">To broaden the readings from just articles or blog posts, this piece is a transcription of a debate during the Boston Area Days of Digital Humanities Conference at Northeastern University in 2013. The discussion centers around the idea of scale, and how it (whether it be broad or narrow scale) is integral to think about for digital humanities research. The combination of presentation slides and recorded discussion underline the interconnection and interdependence of both macro- and microscopic views of DH projects.</p>
      <br/>
      <p class="sources-hangindent"><b>Ramsay, Stephen. “<a href="https://doi.org/10.3998/dh.12544152.0001.001">The Hermeneutics of Screwing Around; or What You Do with a Million Books</a>.” <i>Pastplay: Teaching and Learning History with Technology</i>, edited by Kevin Kee, University of Michigan Press, 2014.</b></p>
      <p class="sources-textindent">The least technical of these essays, Ramsay focuses on the notion that there have always been too many books for humans to read in our lifetime and thus always lists circulating to suggest which books are worth reading. Ramsay sees these lists as paths through culture; or, in other words, a way for people to place a book within a network of their known associations before they read it. Searching (say, via Google) will also reify this known network of association; whereas the act of browsing (like, in a library) allows a person to uncover unknown associations. Ramsay hopes for an algorithmic cataloguing of the digitized books that allows for browsing, or “screwing around.” However, Ramsey acknowledges that humanists are concerned with shared culture, especially in the public sphere, and hopes that digitization and algorithmic efforts can balance these different paths through culture.</p>
      <br/>
      <p class="sources-hangindent"><b>Rawson, Katie, and Trevor Muñoz. "<a href="http://www.curatingmenus.org/articles/against-cleaning/">Against Cleaning</a>." July 2016. <i>curatingmenus.org</b></i></p>
      <p class="sources-textindent">The term “data cleaning” makes opaque the actual process of data transformation. Cleaning implies there is an underlying standard order that needs to be discovered, and all a practitioner has to do is “clean” the “messy” data. Rawson and Muñoz also argue that practitioners who do this data transformation process may not be thinking critically about how they are transforming and standardizing the data, including what valuable data they might be removing. Rawon and Muñoz advocate for centering diversity in data, making transparent the data transformation process through index-making, sharing “messy” data, and bringing communities who are directly affected by or have interest in this data into conversations about data transformation.</p>
    <br/>
      <p class="sources-hangindent"><b>Schöch, Christof. “<a href="http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/">Big? Smart? Clean? Messy? Data in the Humanities</a>.” <i>Journal of Digital Humanities</i>, vol. 2, no. 3, 2013.</b></p>
      <p class="sources-textindent">This reading provides an introduction to defining and understanding “data” in the humanities. In its most basic definition, data in the humanities are digitized and publicized objects of study for humanistic inquiry. Schöch proposes two types of data in the humanities: “smart data” and “big data.” Smart data is data that has been transformed from its original form from which it was collected; Schöch provides TEI encoded documents as an example of smart data. He also argues that the term big data is more of a paradigmatic shift in humanistic inquiry, in which instead of looking at just a few texts, “macroanalysis” (also called distant reading, corpus analysis, etc.) can be performed on a corpus. Schöch concludes with advocates for what he calls “smart big data”: better quantitative processes when performing humanistic inquiries. The “NAME OF OUR WWO PROJECT” is a form of “smart big data,” merging “big data” macroanalysis on a “smart data” corpus.</p>
      <br/>
      <p class="sources-hangindent"><b>Witmore, Michael. “<a href="http://dhdebates.gc.cuny.edu/debates/text/28">Text: A Massively Addressable Object</a>.” <i>Debates in the Digital Humanities</i>, 2012th ed., 2012.</b></p>
      <p class="sources-textindent">Witmore argues that the feature distinguishing digital texts from their physical counterparts is their ability to be examined, or “addressed” at a wide varieties of scale and levels of abstraction. These levels are numerous: to look at a text/texts through a word, a single folio-style book, a genre is to look according to different, flexible scales of abstraction. Although this addressibility is not unique to the digital text, the ease and speed of querying this flexibility is.</p>
      <br/>
    </div>
    &nbsp;
    <button class="sources-collapsible">Digital Pedagogy Readings</button>
    <div class="sources-content">
      <br/>
      <p class="sources-hangindent"><b>Christian-Lamb, Caitlin, and Anelise Hanson Shrout. “<a href="http://www.digitalhumanities.org/dhq/vol/11/3/000311/000311.html">‘Starting From Scratch’? Workshopping New Directions in Undergraduate Digital Humanities</a>.” <i>Digital Humanities Quarterly</i>, vol. 11, no. 3, 2017.</b></p>
      <p class="sources-textindent">This article reports on a workshop about developing undergraduate DH curriculum and courses from the Alliance of Digital Humanities Association’s conference. This workshop discussed and produced the patterns for a successful undergraduate DH curriculum that centers student-agency: these curriculum typically a) emphasize collaboration, b) are housed in traditional spaces that integrate liberal arts pedagogy, and c) are highly flexible. The authors push against terms like “digital native" and "apprentice-researcher" because these models do not accurately capture students’ relationship to technology and can continue to provide misconceptions about DH classrooms on an institutional level. This article demonstrates what digital humanists value in undergraduate education. </p>
      <br/>
      <p class="sources-hangindent"><b>Davis, Rebecca Frost, et al., editors. <i><a href="https://digitalpedagogy.mla.hcommons.org/">Digital Pedagogy in the Humanities: Concepts, Models, and Experiments</a></i>. Modern Language Association Commons, 2016.</b></p>
      <p class="sources-textindent">This piece is a collection of concepts and applied examples from well-known practitioners about incorporating the digital into their pedagogy. These concepts range from artifact-specific terms such as “Visualizations” and “ePortfolios;” to pedagogy-inspired terms like “Collaboration” and “Assessment; to justice-centered concepts such as “Race” and Queer.” When applying this project to your classroom specifically, we might recommend looking at terms such as “Visualization,” “Code,” and “Text Analysis.”</p>
      <br/>
      <p class="sources-hangindent"><b>Harris, Katherine D. “<a href="https://ojcs.siue.edu/ojs/index.php/polymath/article/view/2853">Play, Collaborate, Break, Build, Share: ‘Screwing Around’ in Digital Pedagogy</a>.” <i>Polymath: An Interdisciplinary Arts &amp; Sciences Journal</i>, vol. 3, no. 3, 2013.</b></p>
      <p class="sources-textindent">This article is useful reflection and case study that provides an important and accessible set of guidelines and questions for instructors who want to teach Digital Humanities and integrate digital pedagogy into their classroom. Harris explores the intersections between values in digital pedagogy and digital humanities: collaboration, playfulness, “students” vs “learners,” and more. Harris then demonstrates how she applies these concepts in her undergraduate and graduate class as well as how she assessed learners. </p>
      <br/>
      <p class="sources-hangindent"><b>Sayers, Jentery. “<a href="http://www.doabooks.org/doab?func=fulltext&rid=14815">Tinker-Centric Pedagogy in Literature and Language Classrooms</a>.” <i>Collaborative Approaches to the Digital in English Studies</i>, edited by Laura McGrath, Utah State University Press: Computers and Composition Digital Press, 2011.</b></p>
      <p class="sources-textindent">Sayers advocates for a tinker-centric pedagogy, a learning style that centers around playing, collaborating, and venturing into unfamiliar territories of knowledge. In order to provide evidence for the effectiveness and power of this pedagogy, Sayers provides examples and rationales from his own classrooms. Sayers stresses the importance of assigning “change logs,” or reflections that student writers construct about how their “big ideas” change “from experiment to experiment” (285). The rationales behind each assignment/lesson and the encouragement for researchers and teachers to continue studying the actual implementation of these assignments demonstrates the productive justification for welcoming play in pedagogy.</p>
      <br/>
      <p class="sources-hangindent"><b>Singer, Kate. “<a href="https://jitp.commons.gc.cuny.edu/digital-close-reading-tei-for-teaching-poetic-vocabularies/">Digital Close Reading: TEI for Teaching Poetic Vocabularies</a>.” <i>The Journal of Interactive Technology and Pedagogy</i>, vol. 3, May 2013.</b></p>
      <p class="sources-textindent">This article provides one approach to using TEI in the classroom; Sayer focuses on individual TEI documents and encoding practices instead of using word embeddings to analyze a large TEI document corpus. Singer uses TEI as a pedagogical tool to encourage slow, deliberate, and close reading of poetry. Using TEI invites students to not only understand poetic terminology but actively engage with and use it while reading. Singer provides a narrative for how she incorporated TEI in the classroom, how she plans to revise and play with the structure, and what her assignments and daily lessons during this particular class looked like. Classroom discussions also incorporate visualizations of the student writers TEI-encoded documents, which allows student writers/encoders to engage with each other’s readings and interpretations of texts.</p>
      <br/>
    </div>
  </div>

  <script>
var coll = document.getElementsByClassName("sources-collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    }
  });
}
</script>

</body>
</html>
